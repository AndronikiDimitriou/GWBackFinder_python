{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Example of generating data and training the AA channel<br>\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  Activating project at `~/Documents/Androniki/Github/GWBackFinder.jl`\n","[ Info: Precompiling GWBackFinder [7fa44c00-b40e-49bf-9bf3-4f6997ab6254]\n","/home/zaldivar/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from multiprocessing import cpu_count\n","import os\n","os.environ[\"JULIA_NUM_THREADS\"] = str(cpu_count())\n","from juliacall import Main as jl\n","jl.Pkg.activate(\"/home/zaldivar/Documents/Androniki/Github/GWBackFinder.jl\")\n","jl.seval(\"using GWBackFinder\")\n","from tqdm import tqdm\n","import sys\n","import numpy as np\n","sys.path.append(\"/home/zaldivar/Documents/Androniki/Github/GWBackFinder_python\")\n","from src.GWBackFinder import train as GW_train\n","from src.GWBackFinder import prior as GW_prior\n","import torch"]},{"cell_type":"markdown","metadata":{},"source":["\n","#### load the prior and sample 1 million points"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/zaldivar/.local/lib/python3.9/site-packages/sbi/utils/user_input_checks_utils.py:69: UserWarning: Prior is lacking mean attribute, estimating prior mean from samples.\n","  warnings.warn(\n","/home/zaldivar/.local/lib/python3.9/site-packages/sbi/utils/user_input_checks_utils.py:80: UserWarning: Prior is lacking variance attribute, estimating prior variance from\n","                samples...\n","  warnings.warn(\n"]}],"source":["custom_prior=GW_prior.get_prior()\n","z=custom_prior.sample(1000000)\n","#z=np.save(\"./samples.npy\",z)\n","# %%"]},{"cell_type":"markdown","metadata":{},"source":["### Define frequency range and split in 26 bins of equal log-spacing. $idx_{26}$ shows in which bin the frequency belongs to and $logbins_{26}$ which are the boundaries of the bins. After we coarse grain the frequencies from $3 \\cdot 10^{-3}$ Hz to the maximum frequency 0.5 Hz, i.e., we bin them in 1000 intervals of equal log-spacing. $idx$ shows in which of the 1000 bins the frequency belongs to and $logbins$ the boundaries of the 1000 bins."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["f=jl.range(3*1e-5, 0.5, step=1e-6)\n","idx,idx26,logbins_26,logbins,f_filtered = jl.GWBackFinder.binning(f)\n","Sb1, Sb2, Sb3, Sb4, Sb5, Sb6, Sb7, Sb8, Sb9, Sb10, Sb11, Sb12, Sb13, Sb14, Sb15, Sb16, Sb17, Sb18, Sb19, Sb20, Sb21, Sb22, Sb23, Sb24, Sb25, Sb26 = logbins_26[0],logbins_26[1], logbins_26[2], logbins_26[3], logbins_26[4], logbins_26[5], logbins_26[6], logbins_26[7], logbins_26[8], logbins_26[9], logbins_26[10], logbins_26[11], logbins_26[12], logbins_26[13], logbins_26[14], logbins_26[15], logbins_26[16], logbins_26[17], logbins_26[18], logbins_26[19], logbins_26[20], logbins_26[21], logbins_26[22], logbins_26[23], logbins_26[24], logbins_26[25]"]},{"cell_type":"markdown","metadata":{},"source":["### generate training data using the julia package"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/1 [00:00<?, ?it/s]"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["gw_total_list=[]\n","for i in tqdm(range((1))):\n","    Data_total = jl.GWBackFinder.model_train_data(np.array(z[i,:]), f, idx,\n","                                                  f_filtered,logbins, Sb1, Sb2, Sb3, Sb4, Sb5, Sb6, Sb7, Sb8, Sb9, Sb10, \n","                                                  Sb11, Sb12, Sb13, Sb14, Sb15, Sb16, Sb17, Sb18, Sb19, Sb20, Sb21, Sb22, Sb23, Sb24, Sb25)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["0.003000000000000001"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":[" Data_total "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["0.003000000000000001"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["logbins[0]"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/1000 [00:00<?, ?it/s]"]}],"source":["gw_total_list=[]\n","for i in tqdm(range((1))):\n","    Data_total = jl.GWBackFinder.model_train_data(np.array(z[i,:]), f, idx,\n","                                                  f_filtered,logbins, Sb1, Sb2, Sb3, Sb4, Sb5, Sb6, Sb7, Sb8, Sb9, Sb10, \n","                                                  Sb11, Sb12, Sb13, Sb14, Sb15, Sb16, Sb17, Sb18, Sb19, Sb20, Sb21, Sb22, Sb23, Sb24, Sb25)\n","    gw_total_list.append(np.array(Data_total))"]},{"cell_type":"markdown","metadata":{},"source":["### convert to tensor "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gw_total=torch.tensor(np.array(gw_total_list) , dtype=torch.float32)\n","thetas=torch.tensor(z[0:len(z)])\n","# %%"]},{"cell_type":"markdown","metadata":{},"source":["### check shapes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(gw_total.shape)\n","print(thetas.shape)"]},{"cell_type":"markdown","metadata":{},"source":["\n","### train and save inference in the file train_200.pkl"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["GW_train.train(thetas=thetas, gw_total=gw_total, prior=custom_prior, resume_training=False, validation_fraction=0.2, \n","          learning_rate=1e-4, show_train_summary=True, max_num_epochs=200, \n","          path_saved=None, path_inference=\"/data/users/Androniki/\", name_file=\"train_200.pkl\", model_type=\"nsf\", hidden_features=64, num_transforms=3)"]},{"cell_type":"markdown","metadata":{},"source":["\n","### get posterior from the train_200.pkl and save it in posterior.pkl"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["GW_train.get_posterior(\"/data/users/Androniki/train_200.pkl\",\"posterior.pkl\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":2}
